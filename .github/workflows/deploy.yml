name: Terraform AWS Deployment

on:
  push:
    branches:
      - main
      - develop
      - staging
  pull_request:
    branches: [main, staging]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod
      reset_db_password:
        description: 'Reset database password (use with caution)'
        required: false
        default: false
        type: boolean
      bastion_allowed_cidr:
        description: 'CIDR block allowed to access bastion (e.g. office IP)'
        required: false
        default: '0.0.0.0/0'
        type: string
      wait_for_db:
        description: 'Wait for database to be available before initialization'
        required: false
        default: true
        type: boolean

env:
  TERRAFORM_VERSION: 1.5.7
  PYTHON_VERSION: '3.11'

permissions:
  id-token: write
  contents: read

jobs:
  determine_environment:
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.set-env.outputs.environment }}
      reset_db_password: ${{ steps.set-env.outputs.reset_db_password }}
      wait_for_db: ${{ steps.set-env.outputs.wait_for_db }}
      project_name: ${{ steps.read-vars.outputs.project_name }}
      aws_region: ${{ steps.read-vars.outputs.aws_region }}
      bastion_allowed_cidr: ${{ steps.read-vars.outputs.bastion_allowed_cidr }}
      enable_lifecycle_rules: ${{ steps.read-vars.outputs.enable_lifecycle_rules }}
    steps:
      - uses: actions/checkout@v4
      
      - id: set-env
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          else
            BRANCH="${GITHUB_REF##*/}"
            case "$BRANCH" in
              main) ENV="prod" ;;
              staging) ENV="staging" ;;
              develop) ENV="dev" ;;
              *) ENV="dev" ;;  # fallback default
            esac
            echo "environment=$ENV" >> $GITHUB_OUTPUT
          fi
          
          # Default to NOT resetting DB password unless explicitly triggered
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.reset_db_password }}" == "true" ]]; then
            echo "reset_db_password=true" >> $GITHUB_OUTPUT
          else
            echo "reset_db_password=false" >> $GITHUB_OUTPUT
          fi
          
          # Set wait_for_db flag from input or default to true
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.wait_for_db }}" == "false" ]]; then
            echo "wait_for_db=false" >> $GITHUB_OUTPUT
          else
            echo "wait_for_db=true" >> $GITHUB_OUTPUT
          fi
  
      - id: read-vars
        run: |
          # Determine environment to use
          ENV="${{ steps.set-env.outputs.environment }}"
          echo "Using environment: $ENV"
          
          # Extract project_name and aws_region from terraform.tfvars
          if [ -f "environments/$ENV/terraform.tfvars" ]; then
            PROJECT_NAME=$(grep project_name environments/$ENV/terraform.tfvars | cut -d '=' -f2 | tr -d ' "')
            AWS_REGION=$(grep aws_region environments/$ENV/terraform.tfvars | cut -d '=' -f2 | tr -d ' "')
          else
            echo "Warning: terraform.tfvars file not found for environment $ENV"
          fi
          
          echo "project_name=$PROJECT_NAME" >> $GITHUB_OUTPUT
          echo "aws_region=$AWS_REGION" >> $GITHUB_OUTPUT
          
          # Set S3 lifecycle variables - enable for prod, disable for dev
          if [[ "$ENV" == "prod" ]]; then
            echo "enable_lifecycle_rules=true" >> $GITHUB_OUTPUT
          else
            echo "enable_lifecycle_rules=false" >> $GITHUB_OUTPUT
          fi
          
          # Set bastion allowed CIDR
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && -n "${{ github.event.inputs.bastion_allowed_cidr }}" ]]; then
            echo "bastion_allowed_cidr=${{ github.event.inputs.bastion_allowed_cidr }}" >> $GITHUB_OUTPUT
          else
            # Default to a safe default
            echo "bastion_allowed_cidr=0.0.0.0/0" >> $GITHUB_OUTPUT
          fi

  code-quality:
    name: SonarQube
    runs-on: ubuntu-latest
    needs: [determine_environment]
    env:
      PROJECT_NAME: ${{ needs.determine_environment.outputs.project_name }}
      AWS_REGION: ${{ needs.determine_environment.outputs.aws_region }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      - name: Install tox and any other packages
        run: pip install tox
      - name: Run tox
        run: tox -e py
      - name: SonarQube Scan
        uses: SonarSource/sonarqube-scan-action@v5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
    continue-on-error: true
    
  build_lambda:
    needs: [determine_environment]
    runs-on: ubuntu-latest
    env:
      PROJECT_NAME: ${{ needs.determine_environment.outputs.project_name }}
      AWS_REGION: ${{ needs.determine_environment.outputs.aws_region }}
      STAGE: ${{ needs.determine_environment.outputs.environment }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '${{ env.PYTHON_VERSION }}'
      
      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          
      - name: Package Lambda functions
        run: |
          mkdir -p lambda_artifacts
          
          # Package Lambda functions in a loop to reduce redundancy
          LAMBDA_DIRS=("document_processor" "query_processor" "upload_handler" "db_init" "auth_handler")
          
          for DIR in "${LAMBDA_DIRS[@]}"; do
            if [ -d "src/$DIR" ]; then
              echo "Packaging $DIR Lambda"
              cd src/$DIR && pip install -r requirements.txt -t . || true && zip -r ../../lambda_artifacts/$DIR.zip . && cd ../..
            fi
          done
          
      - name: Upload Lambda artifacts
        uses: actions/upload-artifact@v4
        with:
          name: lambda-artifacts
          path: lambda_artifacts/
          retention-days: 1

  terraform-plan:
    needs: [determine_environment, build_lambda]
    runs-on: ubuntu-latest
    environment: ${{ needs.determine_environment.outputs.environment }}
    env:
      PROJECT_NAME: ${{ needs.determine_environment.outputs.project_name }}
      STAGE: ${{ needs.determine_environment.outputs.environment }}
      AWS_REGION: ${{ needs.determine_environment.outputs.aws_region }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.determine_environment.outputs.aws_region }}

      - name: Create Backend Configuration 
        run: |
          # Create dynamic backend.tf
          cd environments/$STAGE
          
          # Create backend.tf dynamically to use variables
          cat > backend.tf << EOF
          # =========================
          # Terraform Backend Config
          # =========================
          terraform {
            backend "s3" {
              bucket                  = "${PROJECT_NAME}-terraform-state"
              key                     = "${STAGE}/terraform.tfstate"
              region                  = "${AWS_REGION}"
              dynamodb_table          = "${PROJECT_NAME}-${STAGE}-terraform-state-lock"
              encrypt                 = true
          
              # Skip version validation for CI/CD environments
              skip_credentials_validation = true
              skip_metadata_api_check     = true
              force_path_style            = true
            }
          }
          EOF
          
          cd ../..

      - name: Create Terraform State Bucket and DynamoDB Table
        run: |
          # Create the S3 bucket if it doesn't exist
          BUCKET_NAME="${PROJECT_NAME}-terraform-state"
          DYNAMODB_TABLE="${PROJECT_NAME}-${STAGE}-terraform-state-lock"
          
          echo "Creating S3 bucket $BUCKET_NAME if it doesn't exist..."
          aws s3api head-bucket --bucket $BUCKET_NAME 2>/dev/null || aws s3api create-bucket --bucket $BUCKET_NAME --region $AWS_REGION $(if [[ "$AWS_REGION" != "us-east-1" ]]; then echo "--create-bucket-configuration LocationConstraint=$AWS_REGION"; fi)
          
          # Enable versioning on the bucket
          echo "Enabling versioning on bucket $BUCKET_NAME..."
          aws s3api put-bucket-versioning --bucket $BUCKET_NAME --versioning-configuration Status=Enabled
          
          # Create DynamoDB table for state locking if it doesn't exist
          echo "Creating DynamoDB table $DYNAMODB_TABLE if it doesn't exist..."
          aws dynamodb describe-table --table-name $DYNAMODB_TABLE 2>/dev/null || aws dynamodb create-table --table-name $DYNAMODB_TABLE --attribute-definitions AttributeName=LockID,AttributeType=S --key-schema AttributeName=LockID,KeyType=HASH --billing-mode PAY_PER_REQUEST
            
      - name: Download Lambda artifacts
        uses: actions/download-artifact@v4
        with:
          name: lambda-artifacts
          path: lambda_artifacts/

      - name: Upload Lambda ZIP files to S3
        run: |
          LAMBDA_BUCKET_NAME="${PROJECT_NAME}-${STAGE}-lambda-code"

          # Ensure Lambda bucket exists
          aws s3api head-bucket --bucket ${LAMBDA_BUCKET_NAME} 2>/dev/null || \
            aws s3api create-bucket --bucket ${LAMBDA_BUCKET_NAME} \
            --region ${AWS_REGION} $(if [[ "${AWS_REGION}" != "us-east-1" ]]; then echo "--create-bucket-configuration LocationConstraint=${AWS_REGION}"; fi)

          # Upload Lambda ZIP files to correct path in S3
          aws s3 cp lambda_artifacts/ s3://${LAMBDA_BUCKET_NAME}/lambda/ --recursive
          
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
  
      - name: Terraform Init
        working-directory: environments/${{ needs.determine_environment.outputs.environment }}
        run: terraform init
      
      - name: Import Existing Resources
        working-directory: environments/${{ needs.determine_environment.outputs.environment }}
        run: |
          chmod +x ../../scripts/import_resources.sh
          # ../../scripts/import_resources.sh "$PROJECT_NAME" "$STAGE" "$AWS_REGION" > /dev/null 2>&1 || true
          ../../scripts/import_resources.sh "$PROJECT_NAME" "$STAGE" "$AWS_REGION" || true
      
      - name: Terraform Plan
        working-directory: environments/${{ needs.determine_environment.outputs.environment }}
        run: |
          terraform plan \
            -lock=false \
            -var="reset_db_password=${{ needs.determine_environment.outputs.reset_db_password }}" \
            -var="enable_lifecycle_rules=${{ needs.determine_environment.outputs.enable_lifecycle_rules }}" \
            -var="bastion_allowed_cidr=[\"${{ needs.determine_environment.outputs.bastion_allowed_cidr }}\"]" \
            -out=tfplan
        
      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ needs.determine_environment.outputs.environment }}
          path: environments/${{ needs.determine_environment.outputs.environment }}/tfplan
          retention-days: 1

  terraform-apply:
    needs: [determine_environment, build_lambda, terraform-plan]
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    environment: ${{ needs.determine_environment.outputs.environment }}
    env:
      PROJECT_NAME: ${{ needs.determine_environment.outputs.project_name }}
      STAGE: ${{ needs.determine_environment.outputs.environment }}
      AWS_REGION: ${{ needs.determine_environment.outputs.aws_region }}
      RESET_DB_PASSWORD: ${{ needs.determine_environment.outputs.reset_db_password }}
      WAIT_FOR_DB: ${{ needs.determine_environment.outputs.wait_for_db }}
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.determine_environment.outputs.aws_region }}
      
      - name: Download Lambda artifacts
        uses: actions/download-artifact@v4
        with:
          name: lambda-artifacts
          path: lambda_artifacts/
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
      
      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ needs.determine_environment.outputs.environment }}
          path: environments/${{ needs.determine_environment.outputs.environment }}
      
      - name: Terraform Init
        working-directory: environments/${{ needs.determine_environment.outputs.environment }}
        run: terraform init

      - name: Terraform Apply
        working-directory: environments/${{ needs.determine_environment.outputs.environment }}
        run: terraform apply -auto-approve tfplan
      
      - name: Save Terraform Outputs to .env file
        run: |
          echo "COGNITO_CLIENT_ID=$(terraform output -raw cognito_app_client_id)" >> env_vars.env
          echo "API_BASE_URL=$(terraform output -raw api_endpoint)" >> env_vars.env
      
      - name: Upload UI Env File as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: ui-env-vars
          path: env_vars.env

      - name: Wait for RDS instance to be available
        if: ${{ needs.determine_environment.outputs.wait_for_db == 'true' }}
        run: |
          DB_INSTANCE_ID="${PROJECT_NAME}-${STAGE}-postgres"
          
          echo "Waiting for RDS instance $DB_INSTANCE_ID to be available..."
          
          # Wait for up to 10 minutes for the RDS instance to be available
          MAX_ATTEMPTS=60
          ATTEMPT=1
          
          while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
            STATUS=$(aws rds describe-db-instances --db-instance-identifier $DB_INSTANCE_ID --region $AWS_REGION --query "DBInstances[0].DBInstanceStatus" --output text 2>/dev/null || echo "not-found")
            
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS: RDS status is '$STATUS'"
            
            if [ "$STATUS" = "available" ]; then
              echo "RDS instance is available. Proceeding."
              break
            elif [ "$STATUS" = "not-found" ]; then
              echo "RDS instance not found. Waiting..."
            else
              echo "RDS instance is in $STATUS state. Waiting..."
            fi
            
            # If we've reached the maximum attempts, exit with a warning
            if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
              echo "Warning: Maximum attempts reached. RDS instance may not be fully available yet."
              break
            fi
            
            ATTEMPT=$((ATTEMPT+1))
            sleep 10
          done

      - name: Update Lambda env vars if DB password was reset
        if: ${{ needs.determine_environment.outputs.reset_db_password == 'true' }}
        run: |
          # Get the DB credentials secret ARN from terraform output
          cd environments/$STAGE
          DB_SECRET_ARN=$(terraform output -raw db_credentials_secret_arn 2>/dev/null || echo "")
          cd ../..
          
          if [ -n "$DB_SECRET_ARN" ]; then
            echo "Updating Lambda functions with new DB credentials secret ARN..."
            
            # Update all Lambda functions that use the DB credentials
            FUNCTION_NAMES=(
              "${PROJECT_NAME}-${STAGE}-document-processor"
              "${PROJECT_NAME}-${STAGE}-query-processor"
              "${PROJECT_NAME}-${STAGE}-upload-handler"
              "${PROJECT_NAME}-${STAGE}-db-init"
            )
            
            for FUNCTION_NAME in "${FUNCTION_NAMES[@]}"; do
              echo "Updating function: $FUNCTION_NAME"
              # Get current environment variables
              ENV_VARS=$(aws lambda get-function-configuration --function-name "$FUNCTION_NAME" --query "Environment.Variables" || echo "{}")
              
              # Update DB_SECRET_ARN in environment variables if it exists
              if [[ $ENV_VARS == *"DB_SECRET_ARN"* ]]; then
                aws lambda update-function-configuration \
                  --function-name "$FUNCTION_NAME" \
                  --environment "Variables={DB_SECRET_ARN=$DB_SECRET_ARN}"
                echo "Updated DB_SECRET_ARN for $FUNCTION_NAME"
              fi
            done
          else
            echo "DB secret ARN not found, skipping Lambda environment updates"
          fi
      
      - name: Update Lambda functions
        run: |
          # Special handling for db_init function and auth_handler which may not be managed by Terraform
          DB_INIT_NAME="${PROJECT_NAME}-${STAGE}-db-init"
          AUTH_HANDLER_NAME="${PROJECT_NAME}-${STAGE}-auth-handler"
          LAMBDA_BUCKET_NAME="${PROJECT_NAME}-${STAGE}-lambda-code"
          
          # Update the functions that might not be in Terraform
          echo "Updating Lambda functions..."
          aws lambda update-function-code --function-name $DB_INIT_NAME --s3-bucket $LAMBDA_BUCKET_NAME --s3-key "lambda/db_init.zip" || echo "Failed to update db_init, continuing..."
          aws lambda update-function-code --function-name $AUTH_HANDLER_NAME --s3-bucket $LAMBDA_BUCKET_NAME --s3-key "lambda/auth_handler.zip" || echo "Failed to update auth_handler, continuing..."
      
      - name: Invoke Database Init Lambda
        run: |
          DB_INIT_NAME="${PROJECT_NAME}-${STAGE}-db-init"
          
          # Invoke the database initialization Lambda to set up PGVector
          echo "Initializing PostgreSQL database with PGVector..."
          
          # Maximum retry attempts
          MAX_RETRIES=5
          RETRY_COUNT=0
          SUCCESS=false
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$SUCCESS" != "true" ]; do
            RETRY_COUNT=$((RETRY_COUNT+1))
            
            echo "Attempt $RETRY_COUNT to invoke DB init Lambda..."
            RESPONSE=$(aws lambda invoke --function-name $DB_INIT_NAME --payload '{}' /tmp/db_init_output.json 2>&1)
            STATUS_CODE=$?
            
            if [ $STATUS_CODE -eq 0 ]; then
              # Check if the Lambda executed successfully
              FUNCTION_ERROR=$(echo "$RESPONSE" | grep "FunctionError" || echo "")
              
              if [ -z "$FUNCTION_ERROR" ]; then
                echo "DB initialization Lambda invoked successfully"
                cat /tmp/db_init_output.json || echo "Could not get response from DB init Lambda"
                SUCCESS=true
              else
                echo "DB initialization Lambda returned an error. Waiting before retry..."
                cat /tmp/db_init_output.json || echo "Could not get error details"
                sleep 30
              fi
            else
              echo "Failed to invoke DB initialization Lambda. Waiting before retry..."
              echo "Error: $RESPONSE"
              sleep 30
            fi
          done
          
          if [ "$SUCCESS" != "true" ]; then
            echo "Warning: Failed to successfully invoke DB initialization Lambda after $MAX_RETRIES attempts"
            echo "Manual initialization may be required"
          fi

      - name: Verify deployment
        run: |
          UPLOAD_HANDLER_NAME="${PROJECT_NAME}-${STAGE}-upload-handler"
          echo "Verifying deployment by invoking ${UPLOAD_HANDLER_NAME}..."
          aws lambda invoke --function-name $UPLOAD_HANDLER_NAME --payload '{"action": "healthcheck"}' response.json || echo "Verification failed, but deployment may still be successful"
          cat response.json || echo "Could not get response"

  integration_tests:
    needs: [determine_environment, terraform-apply]
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    environment: ${{ needs.determine_environment.outputs.environment }}
    env:
      PROJECT_NAME: ${{ needs.determine_environment.outputs.project_name }}
      STAGE: ${{ needs.determine_environment.outputs.environment }}
      AWS_REGION: ${{ needs.determine_environment.outputs.aws_region }}
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.determine_environment.outputs.aws_region }}
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '${{ env.PYTHON_VERSION }}'
        
      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          fi
        
      - name: Get API endpoint
        id: get-api
        run: |
          # Try to get API endpoint from Terraform output
          cd environments/$STAGE
          API_ENDPOINT=$(terraform output -raw api_endpoint 2>/dev/null || echo "")
          
          if [ -n "$API_ENDPOINT" ] && [ "$API_ENDPOINT" != "" ]; then
            echo "Got API endpoint from Terraform: $API_ENDPOINT"
          else
            echo "Attempting to get API endpoint from AWS CLI..."
            
            # Fallback to using AWS CLI directly if terraform fails
            API_ID=$(aws apigateway get-rest-apis --query "items[?name=='${PROJECT_NAME}-${STAGE}-api'].id" --output text)
            
            if [ -n "$API_ID" ] && [ "$API_ID" != "None" ]; then
              echo "Found API Gateway ID: $API_ID"
              API_ENDPOINT="https://${API_ID}.execute-api.${AWS_REGION}.amazonaws.com/${STAGE}"
              echo "Using API endpoint: $API_ENDPOINT"
            else
              echo "Could not find API Gateway, using example.com fallback"
              API_ENDPOINT="https://example.com"
            fi
          fi
          
          echo "API_ENDPOINT=$API_ENDPOINT" >> $GITHUB_ENV
        
      - name: Check for pytest config files
        run: |
          echo "Looking for pytest configuration files that might interfere with the tests..."
          
          # Check for and temporarily move aside any pytest config files
          CONFIG_FILES=("pytest.ini" "pyproject.toml" ".coveragerc" "conftest.py")
          for FILE in "${CONFIG_FILES[@]}"; do
            if [ -f "$FILE" ]; then
              echo "Found $FILE, temporarily moving it aside for integration tests"
              mv "$FILE" "${FILE}.bak"
            fi
          done
          
          # Check in the src directory too
          if [ -d "src" ] && [ -f "src/conftest.py" ]; then
            echo "Found src/conftest.py, temporarily moving it aside"
            mv "src/conftest.py" "src/conftest.py.bak"
          fi
    
      - name: Run integration tests
        run: |
          echo "Using API endpoint: $API_ENDPOINT"
          
          if [ "$API_ENDPOINT" == "https://example.com" ]; then
            echo "Skipping integration tests as API endpoint could not be determined"
            echo "<testsuites><testsuite name='placeholder'><testcase name='placeholder'><skipped message='API endpoint not available'/></testcase></testsuite></testsuites>" > integration-test-results.xml
          else
            # Use our custom integration test script
            echo "Running custom integration test script..."
            python src/tests/integration/run_integration_tests.py
          fi
        
      - name: Restore config files
        if: always()
        run: |
          echo "Restoring any temporarily moved config files..."
          
          # Restore any temporarily moved pytest config files
          CONFIG_FILES=("pytest.ini" "pyproject.toml" ".coveragerc" "conftest.py")
          for FILE in "${CONFIG_FILES[@]}"; do
            if [ -f "${FILE}.bak" ]; then
              echo "Restoring $FILE"
              mv "${FILE}.bak" "$FILE"
            fi
          done
          
          # Restore in the src directory too
          if [ -d "src" ] && [ -f "src/conftest.py.bak" ]; then
            echo "Restoring src/conftest.py"
            mv "src/conftest.py.bak" "src/conftest.py"
          fi
        
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: integration-test-results.xml